{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqebp8BX_lmO",
        "outputId": "c18100ec-64d2-474c-a9e7-851ab8d8ec18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IDpn7hA0vxqh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.models import load_model\n",
        "from multiprocessing import cpu_count\n",
        "from warnings import catch_warnings\n",
        "from warnings import filterwarnings\n",
        "from pandas import read_csv\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame\n",
        "import time\n",
        "#from datetime import datetime\n",
        "import datetime\n",
        "import itertools\n",
        "import mysql.connector\n",
        "from mysql.connector import connection\n",
        "from mysql.connector import errorcode\n",
        "from statistics import mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iUj-dJTOwa2l"
      },
      "outputs": [],
      "source": [
        "station = ['t51']\n",
        "previous_id = np.zeros(len(station))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hFH57JZowa5J"
      },
      "outputs": [],
      "source": [
        "def loadData(t):\n",
        "    myconn = mysql.connector.connect(host = \"82.180.142.51\",\n",
        "                                database = \"u978805288_land\",\n",
        "                                user = \"u978805288_root\",\n",
        "                                password= \"ACSL@b123\")\n",
        "    (myconn)\n",
        "\n",
        "    sql_select_query = \"select sensor_id, value from data_log, info_time where data_log.info_id = info_time.info_id AND data_log.triplet_id = '\"+t+\"' ORDER BY data_log.info_id desc limit 60\"\n",
        "    date_select_query = \"select Date_Time from info_time where triplet_id='\"+t+\"' order by info_id desc limit 1\"\n",
        "\n",
        "    cursor = myconn.cursor()\n",
        "    cursor.execute(sql_select_query)\n",
        "    data = cursor.fetchall()\n",
        "\n",
        "    cursor.execute(date_select_query)\n",
        "    date = cursor.fetchall()\n",
        "\n",
        "    myconn.close()\n",
        "    return data,date[0][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4hZxJwAVjB_w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data:\n",
            "('s7', '563')\n",
            "('s5', '-0.77,-0.01,-0.45,7.74,4.21,-5.55,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.10')\n",
            "('s1', '1.40,24')\n",
            "('s7', '563')\n",
            "('s5', '-0.77,-0.00,-0.45,9.33,4.15,-5.85,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.14')\n",
            "('s1', '1.70,34')\n",
            "('s7', '562')\n",
            "('s5', '-0.78,-0.01,-0.44,7.87,5.18,-4.63,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.31')\n",
            "('s1', '2.00,29')\n",
            "('s7', '562')\n",
            "('s5', '-0.77,0.00,-0.44,8.17,6.04,-5.91,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.44')\n",
            "('s1', '1.80,30')\n",
            "('s7', '562')\n",
            "('s5', '-0.77,-0.00,-0.44,6.40,5.43,-4.21,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.45')\n",
            "('s1', '2.20,34')\n",
            "('s7', '561')\n",
            "('s5', '-0.77,-0.00,-0.44,7.26,5.00,-4.88,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.39')\n",
            "('s1', '1.70,26')\n",
            "('s7', '561')\n",
            "('s5', '-0.77,-0.00,-0.44,6.83,4.88,-5.67,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.48')\n",
            "('s1', '1.30,29')\n",
            "('s7', '561')\n",
            "('s5', '-0.77,0.00,-0.45,6.71,4.27,-4.76,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.42')\n",
            "('s1', '2.20,27')\n",
            "('s7', '560')\n",
            "('s5', '-0.77,-0.00,-0.45,6.89,5.12,-5.43,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.45')\n",
            "('s1', '1.60,27')\n",
            "('s7', '560')\n",
            "('s5', '-0.77,0.00,-0.43,5.67,5.00,-4.57,0.00,0.00,0.00,0')\n",
            "('s4', '0.00')\n",
            "('s3', '0')\n",
            "('s2', '1054.47')\n",
            "('s1', '2.20,25')\n",
            "Date: 2024-03-14 23:57:13\n"
          ]
        }
      ],
      "source": [
        "triplet_id = \"t51\"\n",
        "data, date_value = loadData(triplet_id)\n",
        "print(\"Data:\")\n",
        "for row in data:\n",
        "    print(row)\n",
        "\n",
        "print(\"Date:\", date_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQaXS7QSp_Is",
        "outputId": "f1ceb569-2104-458a-87b5-49f2de6ee2b0"
      },
      "outputs": [],
      "source": [
        "def dataPreprocess(data):\n",
        "    result = []\n",
        "    data.reverse()\n",
        "\n",
        "    combined_data = []\n",
        "    for i in range(0, len(data), 7):\n",
        "        row = data[i:i+7]\n",
        "        sensor_data = []\n",
        "        for s, d in row:\n",
        "            # print(\"R: \",row)\n",
        "            sid = s\n",
        "            if sid == 's6':\n",
        "                continue\n",
        "            elif sid == 's5':\n",
        "                x = d.split(',')[:6]\n",
        "                for j in x:\n",
        "                    sensor_data.append(float(j))\n",
        "                x = d.split(',')[-1]\n",
        "                sensor_data.append(float(x))\n",
        "            elif sid.startswith('s'):\n",
        "                x = d.split(',')[:]\n",
        "                for j in x:\n",
        "                    sensor_data.append(float(j))\n",
        "\n",
        "        combined_data.extend(sensor_data)\n",
        "\n",
        "    data = np.array(combined_data)\n",
        "    return combined_data\n",
        "\n",
        "\n",
        "preprocessed_data = dataPreprocess(data)\n",
        "split_rows = [preprocessed_data[i:i+13] for i in range(0, len(preprocessed_data), 13)]\n",
        "\n",
        "for row in split_rows:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oItTz886p_N9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "data_list = []\n",
        "count = 0\n",
        "for i in range(10):\n",
        "    d = preprocessed_data[count:count+13]\n",
        "    data_list.append(d)\n",
        "    count += 13\n",
        "\n",
        "data_array = np.array(data_list)\n",
        "data_array = data_array.reshape((-1, 10, 13))\n",
        "data_tensor = tf.convert_to_tensor(data_array, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEZZQNM3g5Fg"
      },
      "outputs": [],
      "source": [
        "# Load the first model\n",
        "STOREDIR = ('/content/drive/MyDrive/Count_data-LMS/urni dhank/Encoder_model/finalmodel/')\n",
        "model1 = tf.saved_model.load(STOREDIR)\n",
        "predict_fn = model1.signatures[\"serving_default\"]\n",
        "\n",
        "# Load the second model\n",
        "STOR = \"/content/drive/MyDrive/Count_data-LMS/urni dhank/\"\n",
        "model2 = tf.keras.models.load_model(STOR+'Encoder_model/website1.h5')\n",
        "\n",
        "def runModel(data_tensor):\n",
        "    previous = 1\n",
        "    try:\n",
        "        # Run the prediction on the first model\n",
        "        pred = predict_fn(data_tensor)\n",
        "        pred = np.argmax(pred)\n",
        "        pred = np.round(pred).astype(int)\n",
        "\n",
        "        # Run the prediction on the second model\n",
        "        dt = data_tensor[:,:,0:5]\n",
        "        pred1 = model2.predict(dt)\n",
        "\n",
        "        combined_list = []\n",
        "        combined_list.extend([round(x,2) for x in pred1.tolist()[0]])\n",
        "        combined_list.append(pred)\n",
        "\n",
        "        return (combined_list)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2BS4hwbvxx0"
      },
      "outputs": [],
      "source": [
        "def insertDatabase(prediction, triplet_id):\n",
        "    connection = mysql.connector.connect(host = \"82.180.142.51\",\n",
        "                                database = \"u978805288_land\",\n",
        "                                user =   \"u978805288_root\",\n",
        "                                password= \"ACSL@b123\")\n",
        "    try:\n",
        "        (connection)\n",
        "        current_time = date+ datetime.timedelta()\n",
        "        print('current_time:',current_time)\n",
        "\n",
        "        val = \"(NULL,'\" +(triplet_id)+\"',\"+ str(prediction[0])+\",\"+ str(prediction[1])+\",\"+str(prediction[2])+\",\"+str(prediction[3])+\",\"+str(prediction[4])+\",\"+str(prediction[5])+ \",'\"+str(current_time)+ \"','no')\"\n",
        "\n",
        "        add_prediction = \"INSERT INTO Prediction_Real_Data  VALUES \"+val\n",
        "        print(add_prediction)\n",
        "\n",
        "        cursor = connection.cursor()\n",
        "        cursor.execute(add_prediction)\n",
        "        connection.commit()\n",
        "        print(cursor.rowcount, \"Record inserted successfully into prediction table\")\n",
        "        cursor.close()\n",
        "\n",
        "    except mysql.connector.Error as error:\n",
        "        print(\"Failed to insert record into prediction table {}\".format(error))\n",
        "\n",
        "    finally:\n",
        "        if connection.is_connected():\n",
        "            connection.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPiCivSW7Bw7"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  count = 0\n",
        "  for triplet_id in station:\n",
        "    data,date = loadData(triplet_id)\n",
        "    print(date)\n",
        "    date = date+ datetime.timedelta(minutes=10)\n",
        "    data = dataPreprocess(data)\n",
        "    data_list = []\n",
        "    count = 0\n",
        "    for i in range(10):\n",
        "        d = preprocessed_data[count:count+13]\n",
        "        data_list.append(d)\n",
        "        count += 13\n",
        "\n",
        "    data_array = np.array(data_list)\n",
        "    data_array = data_array.reshape((-1, 10, 13))\n",
        "    data_tensor = tf.convert_to_tensor(data_array, dtype=tf.float32)\n",
        "    prediction = runModel(data_tensor)\n",
        "    print(prediction)\n",
        "    insertDatabase(prediction,triplet_id)\n",
        "    print('Enjoy')\n",
        "  count+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edIfZm8shTd4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vObrxqIXhThW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TF-Vjpovx4s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeQwQQS98lpy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
