{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hFH57JZowa5J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tensorflow.keras.models import load_model\n",
        "# from multiprocessing import cpu_count\n",
        "# from warnings import catch_warnings\n",
        "# from warnings import filterwarnings\n",
        "# from pandas import read_csv\n",
        "# import matplotlib.pyplot as plt\n",
        "# from pandas import DataFrame\n",
        "# import time\n",
        "# #from datetime import datetime\n",
        "# import datetime\n",
        "# import itertools\n",
        "# import mysql.connector\n",
        "# from mysql.connector import connection\n",
        "# from mysql.connector import errorcode\n",
        "# from statistics import mean\n",
        "# import mysql.connector\n",
        "# import requests\n",
        "# from datetime import datetime\n",
        "# import pytz\n",
        "# import time\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import torch.optim as optim\n",
        "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, mean_squared_error, accuracy_score\n",
        "\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from multiprocessing import cpu_count\n",
        "from warnings import catch_warnings, filterwarnings\n",
        "from pandas import read_csv, DataFrame\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "import mysql.connector\n",
        "from mysql.connector import connection, errorcode\n",
        "from statistics import mean\n",
        "import requests\n",
        "import pytz\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, mean_squared_error, accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData(t):\n",
        "    myconn = mysql.connector.connect(host=\"82.180.142.51\",\n",
        "                                     database=\"u978805288_land\",\n",
        "                                     user=\"u978805288_root\",\n",
        "                                     password=\"ACSL@b123\")\n",
        "    (myconn)\n",
        "\n",
        "    sql_select_query = \"select sensor_id, value from data_log, info_time where data_log.info_id = info_time.info_id AND data_log.triplet_id = '\"+t+\"' ORDER BY data_log.info_id desc limit 70\"\n",
        "    date_select_query = \"select Date_Time from info_time where triplet_id='\"+t+\"' order by info_id desc limit 1\"\n",
        "\n",
        "    cursor = myconn.cursor()\n",
        "    cursor.execute(sql_select_query)\n",
        "    data = cursor.fetchall()\n",
        "\n",
        "    cursor.execute(date_select_query)\n",
        "    date = cursor.fetchall()\n",
        "\n",
        "    myconn.close()\n",
        "    return data, date[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4hZxJwAVjB_w"
      },
      "outputs": [],
      "source": [
        "# triplet_id = \"t39\"\n",
        "# data, date_value = loadData(triplet_id)\n",
        "# print(\"Data:\")\n",
        "# # print(data)\n",
        "# for row in data:\n",
        "#     print(row)\n",
        "\n",
        "# print(\"Date:\", date_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQaXS7QSp_Is",
        "outputId": "f1ceb569-2104-458a-87b5-49f2de6ee2b0"
      },
      "outputs": [],
      "source": [
        "def dataPreprocess(data):\n",
        "    result = []\n",
        "    data.reverse()\n",
        "\n",
        "    combined_data = []\n",
        "    for i in range(0, len(data), 7):\n",
        "        row = data[i:i+7]\n",
        "        sensor_data = []\n",
        "        for s, d in row:\n",
        "            # print(\"R: \",row)\n",
        "            sid = s\n",
        "            if sid == 's6':\n",
        "                continue\n",
        "            elif sid == 's5':\n",
        "                x = d.split(',')[:6]\n",
        "                for j in x:\n",
        "                    sensor_data.append(float(j))\n",
        "                x = d.split(',')[-1]\n",
        "                sensor_data.append(float(x))\n",
        "            elif sid.startswith('s'):\n",
        "                x = d.split(',')[:]\n",
        "                for j in x:\n",
        "                    sensor_data.append(float(j))\n",
        "\n",
        "        combined_data.extend(sensor_data)\n",
        "\n",
        "    data = np.array(combined_data)\n",
        "    return combined_data\n",
        "\n",
        "\n",
        "# preprocessed_data = dataPreprocess(data)\n",
        "# split_rows = [preprocessed_data[i:i+13]\n",
        "#               for i in range(0, len(preprocessed_data), 13)]\n",
        "\n",
        "# for row in split_rows:\n",
        "#     print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oItTz886p_N9"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# data_list = []\n",
        "# count = 0\n",
        "# for i in range(10):\n",
        "#     d = preprocessed_data[count:count+13]\n",
        "#     data_list.append(d)\n",
        "#     count += 13\n",
        "# print(data_list)\n",
        "\n",
        "# data_array = np.array(data_list)\n",
        "# data_array = data_array.reshape((-1, 10, 13))\n",
        "# data_tensor = tf.convert_to_tensor(data_array, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, mean_squared_error, accuracy_score\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the LSTM model for multi-step prediction\n",
        "class MultiStepWeatherLandslidePredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, seq_len, num_weather_params, num_classes):\n",
        "        super(MultiStepWeatherLandslidePredictor, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size,\n",
        "                            num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layers for weather parameters regression (10, 30, and 60 minutes ahead)\n",
        "        self.fc_weather_10 = nn.Sequential(\n",
        "            nn.Linear(hidden_size * seq_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_weather_params)\n",
        "        )\n",
        "        self.fc_weather_30 = nn.Sequential(\n",
        "            nn.Linear(hidden_size * seq_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_weather_params)\n",
        "        )\n",
        "        self.fc_weather_60 = nn.Sequential(\n",
        "            nn.Linear(hidden_size * seq_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_weather_params)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers for landslide classification (10, 30, and 60 minutes ahead)\n",
        "        self.fc_landslide_10 = nn.Sequential(\n",
        "            nn.Linear(hidden_size * seq_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "        self.fc_landslide_30 = nn.Sequential(\n",
        "            nn.Linear(hidden_size * seq_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "        self.fc_landslide_60 = nn.Sequential(\n",
        "            nn.Linear(hidden_size * seq_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(\n",
        "            0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(\n",
        "            0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Flatten the output of LSTM to feed into fully connected layers\n",
        "        out_flattened = out.reshape(out.shape[0], -1)\n",
        "\n",
        "        # Predicting weather parameters\n",
        "        weather_output_10 = self.fc_weather_10(out_flattened)\n",
        "        weather_output_30 = self.fc_weather_30(out_flattened)\n",
        "        weather_output_60 = self.fc_weather_60(out_flattened)\n",
        "\n",
        "        # Predicting landslide classes\n",
        "        landslide_output_10 = self.fc_landslide_10(out_flattened)\n",
        "        landslide_output_30 = self.fc_landslide_30(out_flattened)\n",
        "        landslide_output_60 = self.fc_landslide_60(out_flattened)\n",
        "\n",
        "        return (weather_output_10, weather_output_30, weather_output_60,\n",
        "                landslide_output_10, landslide_output_30, landslide_output_60)\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = 13  # Exclude the first column\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "seq_len = 10\n",
        "num_weather_params = 6\n",
        "num_classes = 3\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MultiStepWeatherLandslidePredictor(\n",
        "    input_size, hidden_size, num_layers, seq_len, num_weather_params, num_classes)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entire model loaded from model_weights.pth\n"
          ]
        }
      ],
      "source": [
        "# Load the model state dictionary\n",
        "PATH = \"model_weights.pth\"\n",
        "\n",
        "model = MultiStepWeatherLandslidePredictor(\n",
        "    input_size, hidden_size, num_layers, seq_len, num_weather_params, num_classes)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "print(f\"Entire model loaded from {PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Database credentials\n",
        "connection_config = {\n",
        "    'host': \"82.180.142.51\",\n",
        "    'database': \"u978805288_land\",\n",
        "    'user': \"u978805288_root\",\n",
        "    'password': \"ACSL@b123\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "C2BS4hwbvxx0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import datetime\n",
        "\n",
        "def insertDatabase(prediction, triplet_id):\n",
        "    connection = mysql.connector.connect(**connection_config)\n",
        "    \n",
        "    try:\n",
        "        (connection)\n",
        "\n",
        "        # current_time = date + datetime.timedelta()\n",
        "        current_time = datetime.datetime.now()  # Get the current time\n",
        "        # current_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        next_time_10 = current_time + datetime.timedelta(minutes=10)\n",
        "        next_time_10 = next_time_10.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        # print('current_time:',current_time)\n",
        "        val_10 = \"(NULL,'\" + (triplet_id)+\"',\" + str(prediction[0][0])+\",\" + str(prediction[0][1])+\",\"+str(prediction[0][2])+\",\"+str(\n",
        "            prediction[0][3])+\",\"+str(prediction[0][4])+\",\"+str(prediction[0][5]) + \",'\"+str(next_time_10) + \"','no')\"\n",
        "        add_prediction_10 = \"INSERT INTO Prediction_Real_Data  VALUES \"+val_10\n",
        "        print(add_prediction_10)\n",
        "\n",
        "        next_time_30 = current_time + datetime.timedelta(minutes=30)\n",
        "        next_time_30 = next_time_30.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        # print('current_time:',current_time)\n",
        "        val_30 = \"(NULL,'\" + (triplet_id)+\"',\" + str(prediction[1][0])+\",\" + str(prediction[1][1])+\",\"+str(prediction[1][2])+\",\"+str(\n",
        "            prediction[1][3])+\",\"+str(prediction[1][4])+\",\"+str(prediction[1][5]) + \",'\"+str(next_time_30) + \"','no')\"\n",
        "        add_prediction_30 = \"INSERT INTO Prediction_Real_Data  VALUES \"+val_30\n",
        "        print(add_prediction_30)\n",
        "\n",
        "        next_time_60 = current_time + datetime.timedelta(minutes=60)\n",
        "        next_time_60 = next_time_60.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        # print('current_time:',current_time)\n",
        "        val_60 = \"(NULL,'\" + (triplet_id)+\"',\" + str(prediction[2][0])+\",\" + str(prediction[2][1])+\",\"+str(prediction[2][2])+\",\"+str(\n",
        "            prediction[2][3])+\",\"+str(prediction[2][4])+\",\"+str(prediction[2][5]) + \",'\"+str(next_time_60) + \"','no')\"\n",
        "        add_prediction_60 = \"INSERT INTO Prediction_Real_Data  VALUES \"+val_60\n",
        "        print(add_prediction_60)\n",
        "\n",
        "        cursor = connection.cursor()\n",
        "        cursor.execute(add_prediction_10)\n",
        "        cursor.execute(add_prediction_30)\n",
        "        cursor.execute(add_prediction_60)\n",
        "        connection.commit()\n",
        "\n",
        "        print(cursor.rowcount, \"Record inserted successfully into prediction table\")\n",
        "        cursor.close()\n",
        "\n",
        "\n",
        "    except mysql.connector.Error as error:\n",
        "        print(\"Failed to insert record into prediction table {}\".format(error))\n",
        "        \n",
        "    finally:\n",
        "        if connection.is_connected():\n",
        "            connection.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "station = [\"t39\",\"t56\",\"t61\",\"t72\",\"t79\",\"t101\",\"t103\"]\n",
        "# Location ID dictionary\n",
        "location_id = {\n",
        "    \"t39\": {\"lat\": 31.777, \"lon\": 76.971},  # Griffon_Peak3\n",
        "    \"t56\": {\"lat\": 31.590, \"lon\": 78.300},  # Purbani_kinnaur\n",
        "    \"t61\": {\"lat\": 31.248, \"lon\": 77.089},  # Tattapani_Mandi\n",
        "    \"t72\": {\"lat\": 32.226, \"lon\": 76.326},  # Dharmshala Kangra\n",
        "    \"t79\": {\"lat\": 32.292, \"lon\": 75.996},  # Bariara_Nurpur_Kangra\n",
        "    \"t101\": {\"lat\": 31.104, \"lon\": 77.173},  # shimla\n",
        "    \"t103\": {\"lat\": 31.708, \"lon\": 76.932},  # mandi\n",
        "}\n",
        "\n",
        "#t38, t37, t39, t71"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenWeatherMap API key\n",
        "API_KEY = 'bdc503040a4bbf2de4eeb9eb3ab841ed'\n",
        "\n",
        "def fetch_weather_data(lat, lon):\n",
        "    url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={API_KEY}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "        data = response.json()\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        return None\n",
        "    except requests.exceptions.ConnectionError as conn_err:\n",
        "        print(f\"Connection error occurred: {conn_err}\")\n",
        "        return None\n",
        "    except requests.exceptions.Timeout as timeout_err:\n",
        "        print(f\"Timeout error occurred: {timeout_err}\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"General error occurred: {req_err}\")\n",
        "        return None\n",
        "    except ValueError as json_err:\n",
        "        print(f\"JSON decode error: {json_err}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        timestamp = data['dt']\n",
        "        temp_k = data['main']['temp']\n",
        "        humidity = data['main']['humidity']\n",
        "        pressure = data['main']['pressure']\n",
        "        rain = data.get('rain', {}).get('1h', 0)\n",
        "        clouds = data['clouds']['all']*100  # percentage to number\n",
        "\n",
        "        temp_c = temp_k - 273.15\n",
        "        temp_c = round(temp_c, 2)\n",
        "        ist = pytz.timezone('Asia/Kolkata')\n",
        "        datetime_ist = datetime.datetime.fromtimestamp(timestamp, ist)\n",
        "        human_readable_date = datetime_ist.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        weather_info = {\n",
        "            \"datetime\": human_readable_date,\n",
        "            \"temp\": temp_c,\n",
        "            \"hum\": humidity,\n",
        "            \"press\": pressure,\n",
        "            \"rain\": rain,\n",
        "            \"clouds\": clouds\n",
        "        }\n",
        "    except KeyError as e:\n",
        "        print(f\"Missing data in the API response: {e}\")\n",
        "        return None\n",
        "\n",
        "    return weather_info\n",
        "\n",
        "\n",
        "def insertWeatherApiData(weather_data_api, triplet_id):\n",
        "    if weather_data_api is None:\n",
        "        print(\n",
        "            f\"Skipping database insert for {triplet_id} due to previous error.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        connection = mysql.connector.connect(**connection_config)\n",
        "\n",
        "        cursor = connection.cursor()\n",
        "        insert_query = \"\"\"\n",
        "        INSERT INTO weather_data_api (datetime, triplet_id, temp, hum, press, rain, light)\n",
        "        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
        "        \"\"\"\n",
        "        record = (weather_data_api[\"datetime\"], triplet_id, weather_data_api[\"temp\"], weather_data_api[\"hum\"],\n",
        "                  weather_data_api[\"press\"], weather_data_api[\"rain\"], weather_data_api[\"clouds\"])\n",
        "\n",
        "        cursor.execute(insert_query, record)\n",
        "        connection.commit()\n",
        "\n",
        "        print(f\"Weather API Record inserted successfully for {triplet_id}: {record}\")\n",
        "    except mysql.connector.Error as error:\n",
        "        print(f\"Failed to insert record: {error}\")\n",
        "    finally:\n",
        "        if connection.is_connected():\n",
        "            cursor.close()\n",
        "            connection.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # INSERTING WEATHER API DATA TO DATABASE\n",
        "# while True:\n",
        "#     for triplet_id, coords in location_id.items():\n",
        "#         lat = coords[\"lat\"]\n",
        "#         lon = coords[\"lon\"]\n",
        "#         weather_data_api = fetch_weather_data(lat, lon)\n",
        "\n",
        "#         # Print statement for debugging\n",
        "#         # print(f\"Weather data for {triplet_id}:\")\n",
        "#         # for key, value in weather_data_api.items():\n",
        "#         #     print(f\"{key}: {value}\")\n",
        "#         # print(\"\\n\")\n",
        "\n",
        "#         # Function call to store data\n",
        "#         insertWeatherApiData(weather_data_api, triplet_id)\n",
        "\n",
        "#     print(\"Data fetched and printed. Waiting for 10 minutes before the next run.\")\n",
        "#     time.sleep(600)  # Sleep for 600 seconds (10 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_preprocessed_weather_data(triplet_id):\n",
        "    try:\n",
        "        connection = mysql.connector.connect(**connection_config)\n",
        "        cursor = connection.cursor()\n",
        "        query = \"\"\"\n",
        "        SELECT * FROM weather_data_api\n",
        "        WHERE triplet_id = %s\n",
        "        ORDER BY datetime DESC\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "        cursor.execute(query, (triplet_id,))\n",
        "        rows = cursor.fetchall()\n",
        "        fetched_data = []\n",
        "\n",
        "        for row in rows:\n",
        "            float_row = []\n",
        "            for elem in row:\n",
        "                # Check if element is numeric (excluding datetime objects)\n",
        "                if isinstance(elem, (int, float)):\n",
        "                    float_row.append(float(elem))\n",
        "                elif isinstance(elem, str) and elem.replace('.', '', 1).isdigit():\n",
        "                    float_row.append(float(elem))\n",
        "            fetched_data.append(float_row[-5:])\n",
        "            # print(float_row[-5:])  # Print the last 5 elements of the row\n",
        "\n",
        "        # Convert fetched_data to a NumPy array with dtype=np.float32\n",
        "        fetched_data = np.array(fetched_data, dtype=np.float32)\n",
        "\n",
        "        return fetched_data\n",
        "    except mysql.connector.Error as error:\n",
        "        print(f\"Failed to fetch records: {error}\")\n",
        "    finally:\n",
        "        if connection.is_connected():\n",
        "            cursor.close()\n",
        "            connection.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# location_id = {\n",
        "#     \"t39\": {\"lat\": 31.777, \"lon\": 76.971}  # Griffon_Peak3\n",
        "    \n",
        "# }\n",
        "\n",
        "location_id = {\n",
        "    \"t39\": {\"lat\": 31.777, \"lon\": 76.971},  # Griffon_Peak3\n",
        "    \"t56\": {\"lat\": 31.590, \"lon\": 78.300},  # Purbani_kinnaur\n",
        "    \"t61\": {\"lat\": 31.248, \"lon\": 77.089},  # Tattapani_Mandi\n",
        "    \"t72\": {\"lat\": 32.226, \"lon\": 76.326},  # Dharmshala Kangra\n",
        "    \"t79\": {\"lat\": 32.292, \"lon\": 75.996},  # Bariara_Nurpur_Kangra\n",
        "    \"t101\": {\"lat\": 31.104, \"lon\": 77.173},  # shimla\n",
        "    \"t103\": {\"lat\": 31.708, \"lon\": 76.932},  # mandi\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-10 21:05:16\n",
            "Weather API Record inserted successfully for t39: ('2024-06-10 21:02:53', 't39', 23.15, 24, 1007, 0, 9700)\n",
            "Data fetched and printed. Waiting for 10 minutes before the next run.\n",
            "[[   23.15    24.    1007.       0.    9700.  ]\n",
            " [   23.15    24.    1007.       0.    9700.  ]\n",
            " [   23.69    21.    1006.       0.    6500.  ]\n",
            " [   29.83    15.    1003.       0.   10000.  ]\n",
            " [   21.74    40.    1006.       0.    2300.  ]\n",
            " [   21.74    40.    1006.       0.    2300.  ]\n",
            " [   21.74    40.    1006.       0.    2300.  ]\n",
            " [   26.18    30.    1005.       0.    4100.  ]\n",
            " [   28.38    21.    1003.       0.     300.  ]\n",
            " [   28.38    21.    1003.       0.     300.  ]]\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t39',24.15,71.49,840.04,-0.54,3194.09,0,'2024-06-10 21:16:07','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t39',23.24,71.36,851.33,-1.27,2870.31,0,'2024-06-10 21:36:07','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t39',22.10,72.44,853.07,-0.46,2642.89,0,'2024-06-10 22:06:07','no')\n",
            "1 Record inserted successfully into prediction table\n",
            "Enjoy\n",
            "2024-06-10 20:57:23\n",
            "Weather API Record inserted successfully for t56: ('2024-06-10 21:06:10', 't56', 17.11, 64, 1019, 0, 4200)\n",
            "Data fetched and printed. Waiting for 10 minutes before the next run.\n",
            "[[  17.11   64.   1019.      0.   4200.  ]\n",
            " [  18.17   50.   1015.      0.   4800.  ]\n",
            " [  17.9    52.   1015.      0.   2900.  ]\n",
            " [  17.9    52.   1015.      0.   2900.  ]\n",
            " [  19.63   43.   1014.      0.   4100.  ]\n",
            " [  19.75   42.   1013.      0.   6500.  ]\n",
            " [  19.63   43.   1014.      0.   4100.  ]\n",
            " [  19.63   43.   1014.      0.   4100.  ]\n",
            " [  20.72   40.   1013.      0.   3800.  ]\n",
            " [  20.72   40.   1013.      0.   3800.  ]]\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t56',21.49,72.89,839.45,0.05,2005.78,0,'2024-06-10 21:16:11','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t56',20.60,74.71,846.19,-0.67,1807.40,0,'2024-06-10 21:36:11','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t56',20.12,76.49,853.12,0.43,1511.66,0,'2024-06-10 22:06:11','no')\n",
            "1 Record inserted successfully into prediction table\n",
            "Enjoy\n",
            "2024-06-10 12:31:03\n",
            "Weather API Record inserted successfully for t61: ('2024-06-10 21:06:14', 't61', 28.14, 19, 1006, 0, 6500)\n",
            "Data fetched and printed. Waiting for 10 minutes before the next run.\n",
            "[[  28.14   19.   1006.      0.   6500.  ]\n",
            " [  29.57   24.   1004.      0.   4100.  ]\n",
            " [  29.57   24.   1004.      0.   4100.  ]\n",
            " [  29.57   24.   1004.      0.   4100.  ]\n",
            " [  34.59   15.   1002.      0.    300.  ]\n",
            " [  33.44   20.   1003.      0.   7600.  ]\n",
            " [  33.44   20.   1003.      0.   7600.  ]\n",
            " [  33.44   20.   1003.      0.   7600.  ]\n",
            " [  35.35   16.   1003.      0.   1600.  ]\n",
            " [  35.35   16.   1003.      0.   1600.  ]]\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t61',20.61,76.07,848.26,0.68,1285.54,0,'2024-06-10 21:16:15','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t61',19.77,78.29,854.26,-0.27,1099.75,0,'2024-06-10 21:36:15','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t61',19.97,81.04,858.36,0.66,749.69,0,'2024-06-10 22:06:15','no')\n",
            "1 Record inserted successfully into prediction table\n",
            "Enjoy\n",
            "2024-06-10 20:55:44\n",
            "Weather API Record inserted successfully for t72: ('2024-06-10 21:06:17', 't72', 23.2, 34, 1009, 0, 2400)\n",
            "Data fetched and printed. Waiting for 10 minutes before the next run.\n",
            "[[  23.2    34.   1009.      0.   2400.  ]\n",
            " [  23.75   35.   1007.      0.   2000.  ]\n",
            " [  23.75   35.   1007.      0.   2000.  ]\n",
            " [  23.75   35.   1007.      0.   2000.  ]\n",
            " [  26.9    29.   1006.      0.   2100.  ]\n",
            " [  27.79   26.   1005.      0.    500.  ]\n",
            " [  26.9    29.   1006.      0.   2100.  ]\n",
            " [  27.79   26.   1005.      0.    500.  ]\n",
            " [  28.44   23.   1006.      0.   4100.  ]\n",
            " [  28.44   23.   1006.      0.   4100.  ]]\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t72',23.78,87.71,959.87,0.99,1072.70,0,'2024-06-10 21:16:19','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t72',23.28,89.81,964.54,0.30,912.58,0,'2024-06-10 21:36:19','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t72',24.81,92.79,967.23,0.32,670.64,0,'2024-06-10 22:06:19','no')\n",
            "1 Record inserted successfully into prediction table\n",
            "Enjoy\n",
            "2024-06-10 20:59:13\n",
            "Weather API Record inserted successfully for t79: ('2024-06-10 21:06:21', 't79', 29.72, 17, 1003, 0, 100)\n",
            "Data fetched and printed. Waiting for 10 minutes before the next run.\n",
            "[[  29.72   17.   1003.      0.    100.  ]\n",
            " [  31.28   25.   1002.      0.    200.  ]\n",
            " [  31.74   22.   1001.      0.   4000.  ]\n",
            " [  31.28   25.   1002.      0.    200.  ]\n",
            " [  35.04   19.   1001.      0.      0.  ]\n",
            " [  35.04   19.   1001.      0.      0.  ]\n",
            " [  35.04   19.   1001.      0.      0.  ]\n",
            " [  35.04   19.   1001.      0.      0.  ]\n",
            " [  36.45   16.   1001.      0.   1700.  ]\n",
            " [  36.45   16.   1001.      0.   1700.  ]]\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t79',24.70,90.06,973.87,0.35,1160.32,0,'2024-06-10 21:16:23','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t79',24.60,90.58,972.86,0.39,1153.87,0,'2024-06-10 21:36:23','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t79',25.72,91.99,981.28,0.30,1099.13,0,'2024-06-10 22:06:23','no')\n",
            "1 Record inserted successfully into prediction table\n",
            "Enjoy\n",
            "2024-06-10 20:55:00\n",
            "Weather API Record inserted successfully for t101: ('2024-06-10 21:06:25', 't101', 18.68, 19, 1007, 0, 2900)\n",
            "Data fetched and printed. Waiting for 10 minutes before the next run.\n",
            "[[  18.68   19.   1007.      0.   2900.  ]\n",
            " [  19.75   26.   1005.      0.   1900.  ]\n",
            " [  19.75   26.   1005.      0.   1900.  ]\n",
            " [  19.75   26.   1005.      0.   1900.  ]\n",
            " [  23.88   21.   1004.      0.   3300.  ]\n",
            " [  23.66   21.   1004.      0.   3300.  ]\n",
            " [  23.66   21.   1004.      0.   3300.  ]\n",
            " [  23.66   21.   1004.      0.   3300.  ]\n",
            " [  25.68   18.   1004.      0.   1900.  ]\n",
            " [  26.73   14.   1002.      0.    700.  ]]\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t101',20.69,92.05,966.63,1.64,40.99,0,'2024-06-10 21:16:27','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t101',19.99,92.02,967.61,0.77,-5.50,0,'2024-06-10 21:36:27','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t101',15.48,90.59,971.48,-0.03,-2.68,0,'2024-06-10 22:06:27','no')\n",
            "1 Record inserted successfully into prediction table\n",
            "Enjoy\n",
            "2024-06-10 20:58:31\n",
            "Weather API Record inserted successfully for t103: ('2024-06-10 21:06:29', 't103', 28.4, 21, 1006, 0, 9700)\n",
            "Data fetched and printed. Waiting for 10 minutes before the next run.\n",
            "[[  28.4    21.   1006.      0.   9700.  ]\n",
            " [  27.56   35.   1005.      0.   2500.  ]\n",
            " [  27.56   35.   1005.      0.   2500.  ]\n",
            " [  27.56   35.   1005.      0.   2500.  ]\n",
            " [  32.1    25.   1004.      0.   4100.  ]\n",
            " [  34.05   17.   1002.      0.    400.  ]\n",
            " [  32.1    25.   1004.      0.   4100.  ]\n",
            " [  32.1    25.   1004.      0.   4100.  ]\n",
            " [  34.5    18.   1004.      0.   6300.  ]\n",
            " [  35.71   15.   1002.      0.    300.  ]]\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t103',23.27,89.76,964.12,1.42,646.67,0,'2024-06-10 21:16:30','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t103',22.67,92.07,968.56,0.50,466.84,0,'2024-06-10 21:36:30','no')\n",
            "INSERT INTO Prediction_Real_Data  VALUES (NULL,'t103',21.02,93.62,972.21,0.92,233.46,0,'2024-06-10 22:06:30','no')\n",
            "1 Record inserted successfully into prediction table\n",
            "Enjoy\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    for t_id, coords in location_id.items():\n",
        "        data, date = loadData(t_id)\n",
        "        print(date)\n",
        "        date = date + timedelta(minutes=10)\n",
        "        preprocessed_data = dataPreprocess(data)\n",
        "        data_list = []\n",
        "        count = 0\n",
        "        for i in range(10):\n",
        "            d = preprocessed_data[count:count+13]\n",
        "            data_list.append(d)\n",
        "            count += 13\n",
        "\n",
        "        data_array = np.array(data_list)\n",
        "        \n",
        "        # data_torch = torch.tensor(data_array) \n",
        "        # print(data_torch)\n",
        "        \n",
        "        lat = coords[\"lat\"]\n",
        "        lon = coords[\"lon\"]\n",
        "        weather_data_api = fetch_weather_data(lat, lon)\n",
        "\n",
        "        # Print statement for debugging\n",
        "        # print(f\"Weather data for {triplet_id}:\")\n",
        "        # for key, value in weather_data_api.items():\n",
        "        #     print(f\"{key}: {value}\")\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        # Function call to store data\n",
        "        insertWeatherApiData(weather_data_api, t_id)\n",
        "\n",
        "        print(\"Data fetched and printed. Waiting for 10 minutes before the next run.\")\n",
        "        data_array = data_array.reshape((-1, 10, 13)).astype(np.float32)\n",
        "        weather_data = fetch_preprocessed_weather_data(t_id)\n",
        "        for i in range(10):\n",
        "            for j in range(5):\n",
        "                data_array[0][0][j] = weather_data[0][j]\n",
        "\n",
        "            # print(\"UPDATED:\\n\")\n",
        "            # print(data_array)\n",
        "\n",
        "        # Convert to PyTorch tensor\n",
        "        print(weather_data)\n",
        "        data_torch = torch.tensor(data_array)  \n",
        "        \n",
        "        with torch.no_grad():\n",
        "            (weather_output_10, weather_output_30, weather_output_60,\n",
        "                landslide_output_10, landslide_output_30, landslide_output_60) = model(data_torch)\n",
        "\n",
        "            # Convert outputs to numpy arrays and move to CPU\n",
        "            weather_output_10 = weather_output_10.cpu().numpy()\n",
        "            weather_output_30 = weather_output_30.cpu().numpy()\n",
        "            weather_output_60 = weather_output_60.cpu().numpy()\n",
        "            landslide_output_10 = torch.argmax(\n",
        "                landslide_output_10, dim=1).cpu().numpy()\n",
        "            landslide_output_30 = torch.argmax(\n",
        "                landslide_output_30, dim=1).cpu().numpy()\n",
        "            landslide_output_60 = torch.argmax(\n",
        "                landslide_output_60, dim=1).cpu().numpy()\n",
        "\n",
        "        formatted_data_format_10 = [\n",
        "            [\"{:.2f}\".format(num) for num in row] for row in weather_output_10]\n",
        "        # print(\"Formatted Data (format method):\", formatted_data_format_10)\n",
        "\n",
        "        # print(\"Predicted weather parameters for 30 minutes ahead:\", weather_output_30)\n",
        "        formatted_data_format_30 = [\n",
        "            [\"{:.2f}\".format(num) for num in row] for row in weather_output_30]\n",
        "        # print(\"Formatted Data (format method):\", formatted_data_format_30)\n",
        "\n",
        "        # print(\"Predicted weather parameters for 60 minutes ahead:\", weather_output_60)\n",
        "        formatted_data_format_60 = [\n",
        "            [\"{:.2f}\".format(num) for num in row] for row in weather_output_60]\n",
        "        # print(\"Formatted Data (format method):\", formatted_data_format_60)\n",
        "\n",
        "        prediction = []\n",
        "        prediction.append(formatted_data_format_10[0])\n",
        "        prediction.append(formatted_data_format_30[0])\n",
        "        prediction.append(formatted_data_format_60[0])\n",
        "\n",
        "        prediction[0][5] = landslide_output_10[0]\n",
        "        prediction[1][5] = landslide_output_30[0]\n",
        "        prediction[2][5] = landslide_output_60[0]\n",
        "        # print(\"Predicted landslide class for 10 minutes ahead:\", landslide_output_10)\n",
        "        # print(\"Predicted landslide class for 30 minutes ahead:\", landslide_output_30)\n",
        "        # print(\"Predicted landslide class for 60 minutes ahead:\", landslide_output_60)\n",
        "\n",
        "        # print(\"Prediction: \", prediction[0])\n",
        "        insertDatabase(prediction, t_id)\n",
        "        print('Enjoy')\n",
        "    time.sleep(600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
